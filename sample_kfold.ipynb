{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize, CenterCrop\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from pytz import timezone\n",
    "import datetime as dt\n",
    "\n",
    "import timm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from f1_score import f1_loss\n",
    "\n",
    "# import wandb\n",
    "# wandb.init(project=\"stratified_imgcls\", entity='dayday')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 25\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)  # type: ignore\n",
    "torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "torch.backends.cudnn.benchmark = True  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configurations\n",
    "data_dir = '/opt/ml/input/data/train'\n",
    "img_dir = f'{data_dir}/images'\n",
    "df_path = f'{data_dir}/train.csv'\n",
    "load_path = \"/opt/ml/code/saved/epoch15_batch20_scheduler_resnet101e_pretrained2.pt\"\n",
    "mean, std = (0.5, 0.5, 0.5), (0.2, 0.2, 0.2)\n",
    "log = []\n",
    "c = ''\n",
    "time = (dt.datetime.now().astimezone(timezone(\"Asia/Seoul\")).strftime(\"%Y-%m-%d_%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import *\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "def get_transforms(need=('train', 'val'), img_size=(512, 384), mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246)):\n",
    "    \"\"\"\n",
    "    train 혹은 validation의 augmentation 함수를 정의합니다. train은 데이터에 많은 변형을 주어야하지만, validation에는 최소한의 전처리만 주어져야합니다.\n",
    "    \n",
    "    Args:\n",
    "        need: 'train', 혹은 'val' 혹은 둘 다에 대한 augmentation 함수를 얻을 건지에 대한 옵션입니다.\n",
    "        img_size: Augmentation 이후 얻을 이미지 사이즈입니다.\n",
    "        mean: 이미지를 Normalize할 때 사용될 RGB 평균값입니다.\n",
    "        std: 이미지를 Normalize할 때 사용될 RGB 표준편차입니다.\n",
    "\n",
    "    Returns:\n",
    "        transformations: Augmentation 함수들이 저장된 dictionary 입니다. transformations['train']은 train 데이터에 대한 augmentation 함수가 있습니다.\n",
    "    \"\"\"\n",
    "    transformations = {}\n",
    "    if 'train' in need:\n",
    "        transformations['train'] = Compose([\n",
    "            CenterCrop(img_size[1],img_size[1]),\n",
    "            Resize(224,224),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            ShiftScaleRotate(p=0.5),\n",
    "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "            RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            GaussNoise(p=0.5),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    if 'val' in need:\n",
    "        transformations['val'] = Compose([\n",
    "            CenterCrop(img_size[1],img_size[1]),\n",
    "            Resize(224,224),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    return transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 마스크 여부, 성별, 나이를 mapping할 클래스를 생성합니다.\n",
    "\n",
    "class MaskLabels:\n",
    "    mask = 0\n",
    "    incorrect = 1\n",
    "    normal = 2\n",
    "\n",
    "class GenderLabels:\n",
    "    male = 0\n",
    "    female = 1\n",
    "\n",
    "class AgeGroup:\n",
    "    map_label = lambda x: 0 if int(x) < 30 else 1 if int(x) < 60 else 2\n",
    "\n",
    "\n",
    "\n",
    "class MaskBaseDataset(data.Dataset):\n",
    "    num_classes = 3 * 2 * 3\n",
    "\n",
    "    _file_names = {\n",
    "        \"mask1.jpg\": MaskLabels.mask,\n",
    "        \"mask2.jpg\": MaskLabels.mask,\n",
    "        \"mask3.jpg\": MaskLabels.mask,\n",
    "        \"mask4.jpg\": MaskLabels.mask,\n",
    "        \"mask5.jpg\": MaskLabels.mask,\n",
    "        \"incorrect_mask.jpg\": MaskLabels.incorrect,\n",
    "        \"normal.jpg\": MaskLabels.normal\n",
    "    }\n",
    "\n",
    "    image_paths = []\n",
    "    mask_labels = []\n",
    "    gender_labels = []\n",
    "    age_labels = []\n",
    "\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        MaskBaseDataset을 initialize 합니다.\n",
    "\n",
    "        Args:\n",
    "            img_dir: 학습 이미지 폴더의 root directory 입니다.\n",
    "            transform: Augmentation을 하는 함수입니다.\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.transform = transform\n",
    "\n",
    "        self.setup()\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        \"\"\"\n",
    "        transform 함수를 설정하는 함수입니다.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        \n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        image의 경로와 각 이미지들의 label을 계산하여 저장해두는 함수입니다.\n",
    "        \"\"\"\n",
    "        profiles = os.listdir(self.img_dir)\n",
    "        for profile in profiles:\n",
    "            for file_name, label in self._file_names.items():\n",
    "                img_path = os.path.join(self.img_dir, profile, file_name)  # (resized_data, 000004_male_Asian_54, mask1.jpg)\n",
    "                if os.path.exists(img_path):\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.mask_labels.append(label)\n",
    "\n",
    "                    id, gender, race, age = profile.split(\"_\")\n",
    "                    gender_label = getattr(GenderLabels, gender)\n",
    "                    age_label = AgeGroup.map_label(age)\n",
    "\n",
    "                    self.gender_labels.append(gender_label)\n",
    "                    self.age_labels.append(age_label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        데이터를 불러오는 함수입니다. \n",
    "        데이터셋 class에 데이터 정보가 저장되어 있고, index를 통해 해당 위치에 있는 데이터 정보를 불러옵니다.\n",
    "        \n",
    "        Args:\n",
    "            index: 불러올 데이터의 인덱스값입니다.\n",
    "        \"\"\"\n",
    "        # 이미지를 불러옵니다.\n",
    "        image_path = self.image_paths[index]\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # 레이블을 불러옵니다.\n",
    "        mask_label = self.mask_labels[index]\n",
    "        gender_label = self.gender_labels[index]\n",
    "        age_label = self.age_labels[index]\n",
    "        multi_class_label = mask_label * 6 + gender_label * 3 + age_label\n",
    "        \n",
    "        # 이미지를 Augmentation 시킵니다.\n",
    "        image_transform = self.transform(image=np.array(image))['image']\n",
    "        return image_transform, multi_class_label\n",
    "\n",
    "    def encode_class(self,m,g,a):        \n",
    "        return m*6+g*3+a\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "        \n",
    "    def set_transform(self, transform):\n",
    "        \"\"\"\n",
    "        transform 함수를 설정하는 함수입니다.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image=np.array(image))['image']\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정의한 Augmentation 함수와 Dataset 클래스 객체를 생성합니다.\n",
    "transform = get_transforms(mean=mean, std=std)\n",
    "\n",
    "dataset = MaskBaseDataset(\n",
    "    img_dir=img_dir,\n",
    ")\n",
    "dataset.set_transform(transform['val'])\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "patience = 10\n",
    "accumulation_steps = 2\n",
    "best_val_loss = np.inf\n",
    "\n",
    "test_dir = '/opt/ml/input/data/eval'\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "\n",
    "test_dataset = TestDataset(image_paths, transform)\n",
    "test_dataset.set_transform(transform['val'])\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  load_state  : /opt/ml/code/saved/epoch15_batch20_scheduler_resnet101e_pretrained2.pt\n",
      "Dataset       : MaskBaseDataset\n",
      "Start_time    : 2021-08-29_02:21:25\n",
      "Device        : cuda\n",
      "BATCH_SIZE    : 32\n",
      "NUM_EPOCH     : 1\n"
     ]
    }
   ],
   "source": [
    "# log.append(f'Model         : {res_model.__class__.__name__}')\n",
    "log.append(f'  load_state  : {load_path}')\n",
    "log.append(f'Dataset       : {dataset.__class__.__name__}')\n",
    "# log.append(f'Train_trans   : {TrainTrans.__name__}')\n",
    "# log.append(f'Test_trans    : {TestTrans.__name__}')\n",
    "log.append(f'Start_time    : {time}')\n",
    "log.append(f'Device        : {device}')\n",
    "# log.append(f'CLASS_NUM     : {CLASS_NUM}')\n",
    "# log.append(f'NUM_WORKERS   : {NUM_WORKERS}')\n",
    "log.append(f'BATCH_SIZE    : {batch_size}')\n",
    "log.append(f'NUM_EPOCH     : {num_epochs}')\n",
    "# log.append(f'SAVE_INTERVAL : {SAVE_INTERVAL}')\n",
    "\n",
    "\n",
    "for line in log:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 불러오기\n",
    "df = pd.read_csv('/opt/ml/input/data/train/train.csv')\n",
    "df.head()\n",
    "\n",
    "# 함수 정의\n",
    "def set_gender(gender):\n",
    "    return 0 if gender == 'male' else 1\n",
    "\n",
    "def set_age(age):\n",
    "    if age<30:\n",
    "        return 0\n",
    "    elif age<60:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# 클래스 분류\n",
    "df['gender_age_cls'] = df.apply(lambda x : set_age(x['age']) + 3*set_gender(x['gender']),axis=1)\n",
    "train_df, val_df = train_test_split(df, test_size=150, stratify=df.gender_age_cls,random_state=25)\n",
    "train_df.to_csv('teamtrain.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'You have to pass data to augmentations as named arguments, for example: aug(image=image)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-cc0a41e2db64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_rows\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mn_rows\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_cols\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mn_rows\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_cols\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Label: {labels[i]}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/albumentations/core/transforms_interface.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_apply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to pass data to augmentations as named arguments, for example: aug(image=image)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplied_in_replay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'You have to pass data to augmentations as named arguments, for example: aug(image=image)'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": "<Figure size 1152x1728 with 12 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "team_df = pd.read_csv('/opt/ml/image-classification-level1-25/teamtrain.csv')\n",
    "\n",
    "# dataset 선언\n",
    "transform = get_transforms(mean=mean, std=std)\n",
    "dataset = MaskBaseDataset(\n",
    "    img_dir=img_dir,\n",
    ")\n",
    "dataset.set_transform(transform['val'])\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "for fold, (train_ids, valid_ids) in enumerate(skf.split(team_df, team_df.gender_age_cls)):\n",
    "    if fold>0: break\n",
    "    #Image index\n",
    "    train_image = sum([[x*7+i for i in range(7)] for x in train_ids],[])\n",
    "    valid_image = sum([[x*7+i for i in range(7)] for x in valid_ids],[])\n",
    "\n",
    "    #Dataset\n",
    "    train_dataset = Subset(dataset,train_image)\n",
    "    valid_dataset = Subset(dataset,valid_image)\n",
    "    \n",
    "    #DataLoader\n",
    "    train_loader = data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=12,\n",
    "        shuffle=False\n",
    "    )\n",
    "    valid_loader = data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=12,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # 사진 찍어보기\n",
    "    images, labels = next(iter(valid_loader))\n",
    "\n",
    "    inv_normalize = transforms.Normalize(\n",
    "        mean=[-m / s for m, s in zip(mean, std)],\n",
    "        std=[1 / s for s in std])\n",
    "\n",
    "    n_rows, n_cols = 4, 3\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, sharex=True, sharey=True, figsize=(16, 24))\n",
    "    for i in range(n_rows*n_cols):\n",
    "        axes[i%n_rows][i//(n_cols+1)].imshow(inv_normalize(images[i]).permute(1, 2, 0))\n",
    "        axes[i%n_rows][i//(n_cols+1)].set_title(f'Label: {labels[i]}', color='r')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 아래에 실제 학습 코드 작성하겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "*** Epoch 0 ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616474cabd774bfb8bdd11c196559e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=451.0), HTML(value='')))"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wandb init\n",
    "config={\"epochs\": num_epochs, \"batch_size\": batch_size, \"learning_rate\" : learning_rate}\n",
    "\n",
    "n_fold = 5\n",
    "skf = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=SEED)\n",
    "\n",
    "all_preds = [0 for _ in range(len(test_dataset))]\n",
    "labels = [dataset.encode_class(m,g,a) for m,g,a in zip(dataset.mask_labels, dataset.gender_labels, dataset.age_labels)]\n",
    "for fold, (train_ids, val_ids) in enumerate(skf.split(dataset.image_paths, labels)):\n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    log.append(f'{c:#^80}')\n",
    "    log.append(f'FOLD {fold}')\n",
    "    log.append(c)\n",
    "\n",
    "    counter = 0\n",
    "    best_val_acc = 0\n",
    "    \n",
    "    # fold별 dataset 정의\n",
    "    train_dataset = Subset(dataset, train_ids)\n",
    "    val_dataset = Subset(dataset, val_ids)\n",
    "\n",
    "    log.append(f'  train_len    {len(train_dataset)}')\n",
    "    log.append(f'  test_len     {len(val_dataset)}')\n",
    "    log.append(c)\n",
    "    # train_dataset.dataset.set_transform(transform['train'])\n",
    "    # val_dataset.dataset.set_transform(transform['val'])\n",
    "\n",
    "    res_model = timm.create_model(model_name = \"resnest101e\", num_classes = 18, pretrained = True)\n",
    "    res_model.load_state_dict(torch.load(load_path))\n",
    "    res_model.to(device)\n",
    "\n",
    "    # Loss, optim\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(res_model.parameters(), lr=learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'max',factor = 0.5, patience=2)\n",
    "\n",
    "    train_loader = data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    val_loader = data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # wandb.init(project=\"stratified_imgcls\", config=config)\n",
    "    # wandb.run.name = f'{fold}fold_{num_epochs}epoch_{batch_size}batch_resnest101e'\n",
    " \n",
    "    for epoch in range(5):# num_epochs):\n",
    "        print('*** Epoch {} ***'.format(epoch))\n",
    "\n",
    "        log.append(c)\n",
    "        log.append('*** Epoch {} ***'.format(epoch))\n",
    "        log.append(c)\n",
    "        # Training\n",
    "        res_model.train()  \n",
    "        running_loss, running_acc, running_f1 = 0.0, 0.0, 0.0\n",
    "            \n",
    "        for idx, (inputs, labels) in enumerate(tqdm(train_loader)):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            with torch.set_grad_enabled(True):\n",
    "                logits = res_model(inputs)\n",
    "                _, preds = torch.max(logits, 1)\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.shape[0]\n",
    "                running_acc += torch.sum(preds == labels.data)\n",
    "                running_f1 += f1_score(labels.data.cpu().numpy(), preds.cpu().numpy(), average = 'macro')\n",
    "                # f1_loss(labels.data, preds)\n",
    "\n",
    "        epoch_acc = running_acc/len(train_loader.dataset)\n",
    "        epoch_loss = running_loss/len(train_loader.dataset)\n",
    "        epoch_f1 = running_f1/len(train_loader)\n",
    "        print('{} Loss: {:.4f} Acc: {:.4f} F1-score: {:.4f}'.format('train', epoch_loss, epoch_acc, epoch_f1))\n",
    "        log.append(f\"Train : [FOLD {fold}] Epoch {epoch:0>3d} // (avg) Loss : {epoch_loss:.3f}, Accuracy : {epoch_acc:.3f}, F1 : {epoch_f1:.3f}\")\n",
    "           \n",
    "        # Validation\n",
    "        res_model.eval()\n",
    "        valid_acc, valid_f1,valid_loss = 0.0, 0.0, 0.0\n",
    "            \n",
    "        for idx, (inputs, labels) in enumerate(tqdm(val_loader)):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with torch.set_grad_enabled(False):\n",
    "                logits = res_model(inputs)\n",
    "                _, preds = torch.max(logits, 1)\n",
    "\n",
    "                # statistics\n",
    "                valid_acc += torch.sum(preds == labels.data)\n",
    "                valid_f1 += f1_score(labels.data.cpu().numpy(), preds.cpu().numpy(), average = 'macro')\n",
    "                valid_loss += criterion(logits, labels).item()\n",
    "\n",
    "        # val_acc /= (idx+1) * batch_size\n",
    "        # val_f1 /= (idx+1) * batch_size\n",
    "        # val_loss /= (idx+1) * batch_size\n",
    "\n",
    "        valid_acc /= len(val_loader.dataset)\n",
    "        valid_f1 /= len(val_loader)\n",
    "        valid_loss /= len(val_loader.dataset)\n",
    "\n",
    "        scheduler.step(valid_acc)\n",
    "\n",
    "        if valid_loss < best_val_loss:\n",
    "                best_val_loss = valid_loss\n",
    "        if valid_acc > best_val_acc:\n",
    "            print(\"New best model for val accuracy!\")\n",
    "            print(f\"val_acc : {valid_acc}\")\n",
    "            torch.save(res_model.state_dict(), f\"saved/{fold}fold_{epoch:02}_accuracy_{valid_acc:4.2%}.ckpt\")\n",
    "            best_val_acc = valid_acc\n",
    "            counter = 0\n",
    "            log.append(\"New best model for val accuracy! val_acc : {valid_acc}\")\n",
    "        else:\n",
    "            counter += 1\n",
    "        # Callback2: patience 횟수 동안 성능 향상이 없을 경우 학습을 종료시킵니다.\n",
    "        if counter > patience:\n",
    "            print(\"Early Stopping...\")\n",
    "            log.append('Early Stopping')\n",
    "            log.append(c)\n",
    "            break\n",
    "\n",
    "\n",
    "        print('{} Acc: {:.4f} f1-score: {:.4f}\\n'.format('valid', valid_acc, valid_f1))\n",
    "        log.append(f\"Valid : [FOLD {fold}] Epoch {epoch:0>3d} // (avg) Loss : {valid_loss:.3f}, Accuracy : {valid_acc:.3f}, F1 : {valid_f1:.3f}\")\n",
    "        log.append(c)\n",
    "        # wandb.log({'train_acc': epoch_acc ,'val_acc': val_acc})# 'train_f1': epoch_f1, 'val_f1':val_f1})\n",
    "\n",
    "\n",
    "    # pred    \n",
    "    all_predictions = []\n",
    "    for images in tqdm(test_loader):\n",
    "        with torch.no_grad():\n",
    "            images = images.to(device)\n",
    "            outputs = res_model(images)\n",
    "            all_predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "    all_preds = [x+y for x,y in zip(all_preds,all_predictions)]\n",
    "\n",
    "submission['ans'] = np.argmax(all_preds,axis = 1)\n",
    "submission.to_csv(os.path.join(test_dir, 'submission_30epoch_4batch_stratified_split5.csv'), index=False)\n",
    "print('test inference is done!')\n",
    "log.append(c)\n",
    "log.append(\"submission file saved\")\n",
    "log.append(c)\n",
    "# log 저장\n",
    "with open(os.path.join('/opt/ml/code/saved', f'{time}.log'), \"w\") as f:\n",
    "    now = (dt.datetime.now().astimezone(timezone(\"Asia/Seoul\")).strftime(\"%Y%m%d_%H%M%S\"))\n",
    "    log.append(f'Finish_Date    : {now}')\n",
    "    print(log[-1])\n",
    "    for line in log: \n",
    "        f.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "submission.to_csv(os.path.join(test_dir, 'submission_30epoch_4batch_stratified_split5.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}